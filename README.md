# Board Scraper

FastAPI ê¸°ë°˜ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œìœ¼ë¡œ, ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“‘ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ì†Œê°œ](#1-í”„ë¡œì íŠ¸-ì†Œê°œ)
2. [ì„œë²„ ì•„í‚¤í…ì²˜](#2-ì„œë²„-ì•„í‚¤í…ì²˜)
3. [ìŠ¤í¬ë˜í¼ ì¶”ê°€ ë°©ë²•](#3-ìŠ¤í¬ë˜í¼-ì¶”ê°€-ë°©ë²•)
4. [ê°œë°œ ê°€ì´ë“œë¼ì¸](#4-ê°œë°œ-ê°€ì´ë“œë¼ì¸)
5. [ì‹œì‘í•˜ê¸°](#5-ì‹œì‘í•˜ê¸°)
6. [API ì—”ë“œí¬ì¸íŠ¸](#6-api-ì—”ë“œí¬ì¸íŠ¸)

## 1. í”„ë¡œì íŠ¸ ì†Œê°œ

- **ëª©ì **: ëŒ€í•™êµ ê²Œì‹œíŒì˜ ê³µì§€ì‚¬í•­ë“¤ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ í†µí•© ê´€ë¦¬
- **íŠ¹ì§•**: DDD ì•„í‚¤í…ì²˜ ì ìš©ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì§€ë³´ìˆ˜ ìš©ì´í•œ êµ¬ì¡°
- **ê¸°ìˆ **: FastAPI, SQLAlchemy, APSchedulerë¥¼ í™œìš©í•œ ë¹„ë™ê¸° ì²˜ë¦¬

### ì£¼ìš” ê¸°ëŠ¥

- ğŸ”„ **ìë™ ìŠ¤í¬ë˜í•‘**: ì„¤ì •ëœ ì£¼ê¸°ë§ˆë‹¤ ìë™ìœ¼ë¡œ ê²Œì‹œíŒ ë°ì´í„° ìˆ˜ì§‘
- ğŸ“Š **ì¤‘ë³µ ê²€ì‚¬**: ê¸°ì¡´ ê²Œì‹œë¬¼ê³¼ ì‹ ê·œ ê²Œì‹œë¬¼ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
- ğŸ—„ï¸ **ë°ì´í„° ê´€ë¦¬**: MySQL ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•œ ì²´ê³„ì ì¸ ë°ì´í„° ì €ì¥
- ğŸ“ˆ **í™•ì¥ì„±**: ìƒˆë¡œìš´ ê²Œì‹œíŒ ì¶”ê°€ê°€ ê°„í¸í•œ ëª¨ë“ˆí˜• êµ¬ì¡°

## 2. ì„œë²„ ì•„í‚¤í…ì²˜

### DDD(Domain-Driven Design) ê³„ì¸µ êµ¬ì¡°

![image](https://github.com/user-attachments/assets/accdd58e-8537-4fd4-a0c7-ed61a2a1df98)

```
app/
â”œâ”€â”€ main.py                     # FastAPI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ config/                     # ì„¤ì • ê´€ë¦¬
â””â”€â”€ board/
    â”œâ”€â”€ domain/                 # ğŸŸ¢ ë„ë©”ì¸ ê³„ì¸µ
    â”‚   â””â”€â”€ repository/        # Repository ì¸í„°í˜ì´ìŠ¤
    â”œâ”€â”€ application/           # ğŸŸ¡ ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ
    â”‚   â””â”€â”€ handlers/          # ìœ ìŠ¤ì¼€ì´ìŠ¤ ì²˜ë¦¬
    â””â”€â”€ infra/                 # ğŸ”´ ì¸í”„ë¼ ê³„ì¸µ
        â”œâ”€â”€ repository/        # Repository êµ¬í˜„ì²´
        â”œâ”€â”€ scraper/          # ìŠ¤í¬ë˜í¼ ê´€ë ¨
        â”œâ”€â”€ schedulers/       # ìŠ¤ì¼€ì¤„ë§ ê´€ë¦¬
        â””â”€â”€ db_models/        # SQLAlchemy ëª¨ë¸
```

### ê³„ì¸µë³„ ì±…ì„

#### ğŸŸ¢ Domain Layer (ë„ë©”ì¸ ê³„ì¸µ)

- **Post ì—”í‹°í‹°**: ê²Œì‹œë¬¼ì˜ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ê³¼ ë¡œì§
- **Repository ì¸í„°í˜ì´ìŠ¤**: ë°ì´í„° ì €ì¥ì†Œì— ëŒ€í•œ ì¶”ìƒí™”
- **ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§**: ì™¸ë¶€ ì˜ì¡´ì„± ì—†ëŠ” ë„ë©”ì¸ ì§€ì‹

#### ğŸŸ¡ Application Layer (ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ)

- **Handler**: ìœ ìŠ¤ì¼€ì´ìŠ¤ ì¡°ìœ¨ ë° ë¹„ì¦ˆë‹ˆìŠ¤ í”Œë¡œìš° ê´€ë¦¬
- **ì„œë¹„ìŠ¤ ì¡°í•©**: ì—¬ëŸ¬ ë„ë©”ì¸ ì„œë¹„ìŠ¤ ê°„ì˜ í˜‘ë ¥ ê´€ë¦¬
- **íŠ¸ëœì­ì…˜ ì²˜ë¦¬**: ë°ì´í„° ì¼ê´€ì„± ë³´ì¥

#### ğŸ”´ Infrastructure Layer (ì¸í”„ë¼ ê³„ì¸µ)

- **Repository êµ¬í˜„ì²´**: ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ë¡œì§
- **Scraper**: ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
- **Scheduler**: ì£¼ê¸°ì  ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- **DB Models**: SQLAlchemy ORM ëª¨ë¸

### ë°ì´í„° íë¦„

```
1. ìŠ¤ì¼€ì¤„ëŸ¬ â†’ ìŠ¤í¬ë˜í¼ ì‹¤í–‰
2. ìŠ¤í¬ë˜í¼ â†’ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
3. í•¸ë“¤ëŸ¬ â†’ ì‹ ê·œ/ê¸°ì¡´ ê²Œì‹œë¬¼ ë¶„ë¥˜
4. Repository â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥/ì—…ë°ì´íŠ¸
```

## 3. ìŠ¤í¬ë˜í¼ ì¶”ê°€ ë°©ë²•

ìƒˆë¡œìš´ ê²Œì‹œíŒì˜ ìŠ¤í¬ë˜í¼ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

### Step 1: ì„¤ì • ì¶”ê°€

`app/config/scraper_config.py`ì— ìƒˆë¡œìš´ ìŠ¤í¬ë˜í¼ ì„¤ì •ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
# app/config/scraper_config.py
from pydantic import BaseModel
from typing import Dict, Literal

class ScraperConfig(BaseModel):
    board_id: int
    base_url: str
    params: Dict[str, str | int]
    interval: int
    campus: Literal["sangmyung", "seoul"]

SCRAPER_CONFIGS = {
    # ê¸°ì¡´ ì„¤ì •ë“¤
    "main_board_sangmyung": ScraperConfig(
        board_id=1,
        base_url="https://www.smu.ac.kr/kor/life/notice.do",
        params={
            "srCampus": "smu",
            "mode": "list",
            "articleLimit": 50,
            "article.offset": 0
        },
        interval=3600,  # 1ì‹œê°„
        campus="sangmyung"
    ),

    # ìƒˆë¡œìš´ ìŠ¤í¬ë˜í¼ ì„¤ì • ì¶”ê°€
    "new_board_scraper": ScraperConfig(
        board_id=3,  # ìƒˆë¡œìš´ board_id
        base_url="https://example.ac.kr/notice",
        params={
            "page": 1,
            "limit": 50
        },
        interval=1800,  # 30ë¶„
        campus="seoul"  # ë˜ëŠ” "sangmyung"
    )
}

def get_scraper_config(scraper_name: str) -> ScraperConfig:
    """ìŠ¤í¬ë˜í¼ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return SCRAPER_CONFIGS.get(scraper_name)
```

### Step 2: ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ êµ¬í˜„

`app/board/infra/scraper/`ì— ìƒˆë¡œìš´ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
# app/board/infra/scraper/new_board_scraper.py
from app.board.infra.scraper.board_scraper_base import BoardScraper
from app.config.scraper_config import get_scraper_config

class NewBoardScraper(BoardScraper):
    def __init__(self, config_name: str):
        config = get_scraper_config(config_name)
        self.base_url = config.base_url
        self.board_id = config.board_id
        self.params = config.params

    def scrape(self) -> dict:
        """ìŠ¤í¬ë˜í•‘ ë¡œì§ êµ¬í˜„"""
        # ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ í›„
        # {"board_id": int, "count": int, "data": {post_id: ScrapedPost}} í˜•íƒœë¡œ ë°˜í™˜
        pass
```

### Step 3: ìŠ¤í¬ë˜í¼ ë“±ë¡

`app/board/infra/schedulers/scraper_initializer.py`ì—ì„œ ìƒˆ ìŠ¤í¬ë˜í¼ë¥¼ ë“±ë¡í•©ë‹ˆë‹¤.

```python
# ìƒˆ ìŠ¤í¬ë˜í¼ import ì¶”ê°€
from app.board.infra.scraper.new_board_scraper import NewBoardScraper

def initialize_scrapers(scheduler: BoardScrapeScheduler):
    """ëª¨ë“  ìŠ¤í¬ë˜í¼ë¥¼ í•œë²ˆì— ë“±ë¡"""
    logger.info("ìŠ¤í¬ë˜í¼ ë“±ë¡ ì‹œì‘")

    scrapers = [
        MainBoardScraper("main_board_sangmyung"),
        MainBoardScraper("main_board_seoul"),
        NewBoardScraper("new_board_scraper"),  # ìƒˆ ìŠ¤í¬ë˜í¼ ì¶”ê°€
    ]

    for scraper in scrapers:
        scheduler.add_board_scrape_job(scraper)
        scraper_name = f"{scraper.__class__.__name__}_{getattr(scraper, 'board_id', 'unknown')}"
        logger.info(f"{scraper_name} ë“±ë¡ ì™„ë£Œ")

    logger.info(f"ëª¨ë“  ìŠ¤í¬ë˜í¼ ë“±ë¡ ì™„ë£Œ: ì´ {len(scrapers)}ê°œ")
```

### Step 4: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

ìƒˆë¡œìš´ ê²Œì‹œíŒ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.

```sql
INSERT INTO board (id, campus, site, board_type, url)
VALUES (3, 'seoul', 'Example University', 'notice', 'https://example.ac.kr');
```

### Step 5: ì„œë²„ ì¬ì‹œì‘

```bash
uvicorn app.main:app --reload
```

ì„œë²„ê°€ ì‹œì‘ë˜ë©´ ìƒˆë¡œìš´ ìŠ¤í¬ë˜í¼ê°€ ìë™ìœ¼ë¡œ ë“±ë¡ë˜ê³  ì„¤ì •ëœ ì£¼ê¸°ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.

## 4. ê°œë°œ ê°€ì´ë“œë¼ì¸

### ìŠ¤í¬ë˜í¼ ê°œë°œ ê·œì¹™

1. **BoardScraper ìƒì†**: ëª¨ë“  ìŠ¤í¬ë˜í¼ëŠ” `BoardScraper`ë¥¼ ìƒì†ë°›ì•„ì•¼ í•¨
2. **scrape() ë©”ì„œë“œ êµ¬í˜„**: `{"board_id": int, "count": int, "data": dict}` í˜•ì‹ ë°˜í™˜ í•„ìˆ˜
3. **ScrapedPost ì‚¬ìš©**: ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ `ScrapedPost` ëª¨ë¸ ì‚¬ìš©
4. **ì„¤ì • ê¸°ë°˜**: `get_scraper_config()`ë¥¼ í†µí•´ ì„¤ì • ì •ë³´ ë¡œë“œ

### ì„¤ì • íŒŒë¼ë¯¸í„°

- **board_id**: ë°ì´í„°ë² ì´ìŠ¤ì˜ ê²Œì‹œíŒ ê³ ìœ  ID
- **base_url**: ìŠ¤í¬ë˜í•‘í•  ì›¹ì‚¬ì´íŠ¸ URL
- **params**: ìš”ì²­ì— í•„ìš”í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
- **interval**: ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ê°„ê²© (ì´ˆ ë‹¨ìœ„)
- **campus**: ìº í¼ìŠ¤ êµ¬ë¶„ ("sangmyung" ë˜ëŠ” "seoul")

## 5. ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository-url>
cd board-scraper

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì •

í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

```bash
# .env
DATABASE_URL=mysql+aiomysql://username:password@localhost:3306/database_name
```

**í™˜ê²½ë³€ìˆ˜ ì„¤ëª…:**

- `DATABASE_URL`: MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ìì—´
  - í˜•ì‹: `mysql+aiomysql://ì‚¬ìš©ìëª…:ë¹„ë°€ë²ˆí˜¸@í˜¸ìŠ¤íŠ¸:í¬íŠ¸/ë°ì´í„°ë² ì´ìŠ¤ëª…`
  - ì˜ˆì‹œ: `mysql+aiomysql://root:1234@localhost:3306/board_scraper`

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

```bash
# MySQL ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
mysql -u root -p
CREATE DATABASE board_scraper;

# í…Œì´ë¸” ìƒì„± (SQLAlchemy ëª¨ë¸ ê¸°ë°˜)
# ì„œë²„ ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
```

### 4. ì„œë²„ ì‹¤í–‰

```bash
# ê°œë°œ ì„œë²„ ì‹¤í–‰ (ìë™ ì¬ì‹œì‘)
uvicorn app.main:app --reload --port 8000

# ë˜ëŠ” FastAPI CLI ì‚¬ìš©
fastapi dev app/main.py

# í”„ë¡œë•ì…˜ ì‹¤í–‰
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### 5. ì‹¤í–‰ í™•ì¸

```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health

# ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
http://localhost:8000
```

**ì •ìƒ ì‹¤í–‰ ì‹œ ë¡œê·¸:**

```
INFO: ì„œë²„ ì‹œì‘: ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì¤‘...
INFO: ìŠ¤í¬ë˜í¼ ë“±ë¡ ì‹œì‘
INFO: MainBoardScraper_1 ë“±ë¡ ì™„ë£Œ
INFO: MainBoardScraper_2 ë“±ë¡ ì™„ë£Œ
INFO: ëª¨ë“  ìŠ¤í¬ë˜í¼ ë“±ë¡ ì™„ë£Œ: ì´ 2ê°œ
```

## 6. API ì—”ë“œí¬ì¸íŠ¸

```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
GET /health

# ì‘ë‹µ ì˜ˆì‹œ
{
  "status": "healthy",
  "active_jobs": 2
}
```

```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork í”„ë¡œì íŠ¸
2. Feature ë¸Œëœì¹˜ ìƒì„± (`git checkout -b feature/new-scraper`)
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹ (`git commit -am 'feat:Add new scraper'`)
4. ë¸Œëœì¹˜ì— Push (`git push origin feature/new-scraper`)
5. Pull Request ìƒì„±

## ğŸ“ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
```
